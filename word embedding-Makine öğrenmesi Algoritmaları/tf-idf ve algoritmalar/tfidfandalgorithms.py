# -*- coding: utf-8 -*-
"""tfidf_kitaptürkodlar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19K3aC7l4gYvWQvgJvND-h83hByomuUra
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
import re
import warnings
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
#verinin tanımlanması
data = pd.read_excel('/content/duygular_stemmed.xlsx')
data.head()

data.info()

data.shape

class_ = Counter(data['label']).keys()
class_

import math
dataDoc = data['comments'].values.tolist()
dataClass = data['label'].values.tolist()
#test ve train olarak verinin ayrılması
x_train, x_test, y_train, y_test = train_test_split(dataDoc, dataClass, test_size = 0.3, random_state = 42)
#tfidf işlemi
## max_df 0,9 tfidf skorundan fazla olanları kelimeleri alma
## min_df 5 frekansından düşük olan kelimeleri alma
dataDoc = ["" if isinstance(doc, float) and math.isnan(doc) else doc for doc in dataDoc]
x_train = ["" if isinstance(doc, float) and math.isnan(doc) else doc for doc in x_train]
x_test = ["" if isinstance(doc, float) and math.isnan(doc) else doc for doc in x_test]
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=1)
x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)
x_test_tfidf = tfidf_vectorizer.transform(x_test)
print("Number of documents in x_train_tfidf:", x_train_tfidf.shape)
print("Number of documents in x_test_tfidf:", x_test_tfidf.shape)



words_counts = Counter([word for line in x_train for word in line.split(' ')])
most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:100]
most_common_words[:100]

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC, SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import CategoricalNB
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier  # Importing MLPClassifier
import matplotlib.pyplot as plt
import sklearn


#logistic regresyon
model = OneVsRestClassifier(LogisticRegression(penalty = 'l2', C=1.0))
model.fit(x_train_tfidf, y_train)

print ("Logistic Regression Accuracy={}".format(accuracy_score(y_test, model.predict(x_test_tfidf))))
logisticpred = accuracy_score(y_test, model.predict(x_test_tfidf))
# F score, precision ve recall değerlerini hesaplayalım

f_score = f1_score(y_test, model.predict(x_test_tfidf), average='macro')
precision = precision_score(y_test, model.predict(x_test_tfidf),average='macro')
recall = recall_score(y_test, model.predict(x_test_tfidf), average='macro')
print("F score:", f_score)
print("Precision:", precision)
print("Recall:", recall)


print("   ")

# Naive Bayes
naive_bayes = MultinomialNB()
naive_bayes.fit(x_train_tfidf, y_train)
nb_pred = naive_bayes.predict(x_test_tfidf)

print('Naive Bayes Accuracy: {}'.format(accuracy_score(y_test, nb_pred)))
print("F Score:", f1_score(y_test, nb_pred, average='macro'))
print("Precision:", precision_score(y_test, nb_pred, average='macro'))
print("Recall:", recall_score(y_test, nb_pred, average='macro'))



print(" ")
# Support Vector Machine (SVM)
svm = OneVsRestClassifier(SVC(kernel='linear'))
svm.fit(x_train_tfidf, y_train)
svm_pred = svm.predict(x_test_tfidf)

print('SVM Accuracy: {}'.format(accuracy_score(y_test, svm_pred)))
print("F Score:", f1_score(y_test, svm_pred, average='macro'))
print("Precision:", precision_score(y_test, svm_pred, average='macro'))
print("Recall:", recall_score(y_test, svm_pred, average='macro'))

print(" ")


# Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(x_train_tfidf, y_train)
dt_pred = decision_tree.predict(x_test_tfidf)



print('Decision Tree Accuracy: {}'.format(accuracy_score(y_test, dt_pred)))
print("F Score:", f1_score(y_test, dt_pred, average='macro'))
print("Precision:", precision_score(y_test, dt_pred, average='macro'))
print("Recall:", recall_score(y_test, dt_pred, average='macro'))

print(" ")

# Random Forest
random_forest = RandomForestClassifier()
random_forest.fit(x_train_tfidf, y_train)
rf_pred = random_forest.predict(x_test_tfidf)



print('Random Forest Accuracy: {}'.format(accuracy_score(y_test, rf_pred)))
print("F Score:", f1_score(y_test, rf_pred, average='macro'))
print("Precision:", precision_score(y_test, rf_pred, average='macro'))
print("Recall:", recall_score(y_test, rf_pred, average='macro'))



# Multi layer perceptron (MLP)
mlp = MLPClassifier()
mlp.fit(x_train_tfidf, y_train)
mlp_pred = mlp.predict(x_test_tfidf)



print('MLP Accuracy: {}'.format(accuracy_score(y_test, mlp_pred)))
print("F Score:", f1_score(y_test, mlp_pred, average='macro'))
print("Precision:", precision_score(y_test, mlp_pred, average='macro'))
print("Recall:", recall_score(y_test, mlp_pred, average='macro'))

#Training ML classification models

# Naive Bayes
from sklearn.naive_bayes import GaussianNB, MultinomialNB


# MultinomialNB is used in multi-class classification
#clf = Gaussian#NB()
clf = MultinomialNB()


print(clf)

NB=clf.fit(x_train_tfidf,y_train)
prediction=NB.predict(x_test_tfidf)

# Measuring accuracy on Testing Data
from sklearn import metrics
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score=metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))

# Importing cross validation function from sklearn
#from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
#Accuracy_Values=cross_val_score(NB, X , y, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 5-fold Cross Validation:\n',Accuracy_Values)
#print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))



from sklearn.svm import SVC

# Choose hyperparameters (you can tune these)
svm_clf = SVC(kernel='linear', C=1.0)

# Printing all the parameters of SVM
print(svm_clf)

# Creating the model on Training Data
svm_model = svm_clf.fit(x_train_tfidf, y_train)
prediction_svm = svm_model.predict(x_test_tfidf)

# Measuring accuracy on Testing Data
from sklearn import metrics
print(metrics.classification_report(y_test, prediction_svm))
print(metrics.confusion_matrix(y_test, prediction_svm))

# Printing the Overall Accuracy of the model
f1_score_svm = metrics.f1_score(y_test, prediction_svm, average='weighted')
print('Accuracy of the SVM model on Testing Sample Data:', round(f1_score_svm, 2))

# Importing cross-validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross-validation on the SVM model
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
#accuracy_values_svm = cross_val_score(svm_model, X, y, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 10-fold Cross Validation:\n', accuracy_values_svm)
#print('\nFinal Average Accuracy of the SVM model:', round(accuracy_values_svm.mean(), 2))

# Random Forest
from sklearn.ensemble import RandomForestClassifier

# Choose hyperparameters (you can tune these)
clf = RandomForestClassifier(n_estimators=600, max_depth=40, criterion='gini')

# Printing all the parameters of Random Forest
print(clf)

# Creating the model on Training Data
RF_model = clf.fit(x_train_tfidf, y_train)
prediction = RF_model.predict(x_test_tfidf)

# Measuring accuracy on Testing Data
from sklearn import metrics
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))



# Importing cross-validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross-validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
#Accuracy_Values = cross_val_score(RF_model, X, y, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
#print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(), 2))

# Decision Trees
from sklearn import tree
#choose from different tunable hyper parameters
clf = tree.DecisionTreeClassifier(max_depth=20,criterion='gini')

# Printing all the parameters of Decision Trees
print(clf)

# Creating the model on Training Data
DTree=clf.fit(x_train_tfidf,y_train)
prediction=DTree.predict(x_test_tfidf)

# Measuring accuracy on Testing Data
from sklearn import metrics
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score=metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))



# Importing cross validation function from sklearn
#from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
#Accuracy_Values=cross_val_score(DTree, X , y, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
#print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
import pandas as pd

# Choosing hyperparameters for Logistic Regression
clf = LogisticRegression(C=25, penalty='l2', solver='saga')

# Training the model on training data
LOG = clf.fit(x_train_tfidf, y_train)

# Generating predictions on testing data
prediction = LOG.predict(x_test_tfidf)

# Creating feature names for the DataFrame
feature_names = [f'Feature_{i}' for i in range(x_test_tfidf.shape[1])]

# Printing sample values of predictions in testing data
TestingData = pd.DataFrame(data=x_test_tfidf.toarray(), columns=feature_names)
TestingData['Actual'] = y_test
TestingData['Predicted'] = prediction
print(TestingData.head())

# Measuring accuracy on testing data
print("Classification Report:")
print(classification_report(y_test, prediction))

print("Confusion Matrix:")
print(confusion_matrix(y_test, prediction))

# Calculating F1 Score
f1 = f1_score(y_test, prediction, average='weighted')
print('F1 Score:', round(f1, 2))

# Calculating Accuracy
accuracy = accuracy_score(y_test, prediction)
print('Accuracy:', round(accuracy, 2))


## Importing cross validation function from sklearn
#from sklearn.model_selection import cross_val_score

## Running 10-Fold Cross validation on a given algorithm
## Passing full data X and y because the K-fold will split the data and automatically choose train/test
#Accuracy_Values=cross_val_score(LOG, X , y, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
#print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
from sklearn.model_selection import cross_val_score
import pandas as pd

# Initializing MLP Classifier
clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, activation='relu', solver='adam', random_state=42)
print(clf)

# Training the model on training data
MLP = clf.fit(x_train_tfidf, y_train)

# Generating predictions on testing data
prediction = MLP.predict(x_test_tfidf)

# Measuring accuracy on testing data
print("Classification Report:")
print(classification_report(y_test, prediction))

print("Confusion Matrix:")
print(confusion_matrix(y_test, prediction))

# Calculating F1 Score
f1 = f1_score(y_test, prediction, average='weighted')
print('F1 Score:', round(f1, 2))

# Calculating Accuracy
accuracy = accuracy_score(y_test, prediction)
print('Accuracy:', round(accuracy, 2))

# Running 10-Fold Cross validation on MLP classifier
#Accuracy_Values = cross_val_score(clf, x_train_tfidf, y_train, cv=10, scoring='f1_weighted')
#print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
#print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(), 2))

"""accurancy artırmak ıcın"""



from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV

# Defining the parameter grid
param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}

# Setting up GridSearchCV
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='f1_weighted')

# Fitting the model
grid_search.fit(x_train_tfidf, y_train)

# Best model from grid search
best_clf = grid_search.best_estimator_

print(best_clf)

# Making predictions with the best model
prediction = best_clf.predict(x_test_tfidf)

# Measuring accuracy on Testing Data
from sklearn import metrics
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))



